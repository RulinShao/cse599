---
title: Schedule
layout: home
nav_order: 1
---

<style type="text/css">
  th {
    font-size: 1rem !important;
  }
  td {
    vertical-align: top;
    font-size: 1rem !important;
  }
  .staffer {
    display: flex;
    margin: 1rem;
  }
  .staffer p, .staffer .staffer-name {
    margin: 0.25rem !important;
    margin-top: 0.5rem !important;
  }
  .staffer .staffer-image {
    border-radius: 50%;
    height: 100px;
    margin-right: 1rem;
  }
  .staffer .staffer-meta {
    color: #959396 !important;
    margin-top: 0.1rem !important;
  }
</style>

<h1>CSE599H: Advances and Challenges in Language Models, Reasoning, and AI Agents</h1>
<h3>
Spring 2024-2025<br/>
Mondays and Wednesdays, 3pm to 4:20pm<br/>
CSE2 G04<br/>
<a href="https://www.gradescope.com/courses/1007475">Gradescope</a>
|
<a href="https://edstem.org/us/courses/50883">Ed</a>
</h3>
<br/>

<div class="staffer">
  <img class="staffer-image" src="{{site.url}}/images/hanna.jpg" alt="">
  <div>
    <h3 class="staffer-name">
      <a href="https://homes.cs.washington.edu/~hannaneh/">Hanna Hajishirzi</a>
    </h3>
    <p class="staffer-meta">Instructor</p>
    <p class="staffer-meta"><a href="mailto:hannaneh@cs.washington.edu">hannaneh@cs.washington.edu</a></p>
  </div>
</div>

<div class="staffer">
  <img class="staffer-image" src="{{site.url}}/images/jiacheng.jpg" alt="">
  <div>
    <h3 class="staffer-name">
      <a href="https://liujch1998.github.io/">Jiacheng Liu</a>
    </h3>
    <p class="staffer-meta">TA</p>
    <p class="staffer-meta"><a href="liujc@cs.washington.edu">liujc@cs.washington.edu</a></p>
  </div>
</div>

<div class="staffer">
  <img class="staffer-image" src="{{site.url}}/images/rulin.jpg" alt="">
  <div>
    <h3 class="staffer-name">
      <a href="https://rulinshao.github.io/">Rulin Shao</a>
    </h3>
    <p class="staffer-meta">TA</p>
    <p class="staffer-meta"><a href="rulins@cs.washington.edu">rulins@cs.washington.edu</a></p>
  </div>
</div>

Office hours by appointment. <br/>
To contact course staff, please make an Ed post. <br/>
We welcome feedback on the course. If you prefer to leave it anonymously, use
<a href="https://forms.gle/khU2mGjaaTDF7poa8">this form</a>.


<hr/>

<p>
  Language models, such as GPT-o3, DeepSeek-R1, and Deep Research, have demonstrated remarkable capabilities in natural language understanding, generation, and reasoning, with applications ranging from literature summarization to complex problem-solving tasks. However, as we will discuss, these models are not without limitations, such as susceptibility to hallucinations, poor capabilities in strategic exploration, and limitations in long-horizon planning. In this class, we will explore the latest research on language models, reasoning, and AI agents, discussing both the advances and challenges in these areas. We will examine the current state-of-the-art models, their limitations, and the ongoing efforts to address these challenges. Through this course, you will engage in paper discussions and gain a deeper understanding of the latest developments in the field and contribute to the ongoing discussions and research in this exciting area.
</p>
<p>
  This is a seminar designed for PhD students. Students are expected to be able to read and understand the assigned papers on their own, and they should be familiar with ML and NLP concepts at the level of having taken advanced undergraduate classes.
</p>

<h2>Schedule</h2>

Weekly due dates:
<ul>
  <li>By Monday 11:59pm: Slides for Wednesday's papers (presenters only)</li>
  <li>By Saturday 11:59pm: Slides for Monday's papers (presenters only)</li>
</ul>



<h3>1. Course Overview</h3>


<table>
  <tr>
    <td>Mar 31 (Mon)</td>
    <td> <strong>Course overview
      <!-- <a href="http://localhost:4000/slides/CSE599H_1-5-24.pdf">(slides)</a> -->
  </strong><br>
  <!-- <ul>
      <li><a href="https://arxiv.org/abs/2407.21783">The Llama 3 Herd of Models</a></li>
      <li><a href="https://arxiv.org/abs/2501.00656">2 OLMo 2 Furious</a></li>
      <li><a href="https://arxiv.org/abs/2412.19437">DeepSeek-V3 Technical Report</a></li>
  </ul> <u>Optional reading</u>
  <ul>
      <li><a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook">The Ultra-Scale Playbook:
          Training LLMs on GPU Clusters</a></li>
  </ul> -->
</td>
  </tr>
  <tr>
    <td>Apr 2 (Wed)</td>
    <td><strong>Basic Pre-training and Post-training</strong><br>
      <ul>
        <li><a href="https://arxiv.org/abs/2407.21783">The Llama 3 Herd of Models</a></li>
        <li><a href="https://arxiv.org/abs/2501.00656">2 OLMo 2 Furious</a></li>
        <li><a href="https://arxiv.org/abs/2412.19437">DeepSeek-V3 Technical Report</a></li>
    </ul> <u>Optional reading</u>
    <ul>
        <li><a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook">The Ultra-Scale Playbook:
            Training LLMs on GPU Clusters</a></li>
    </ul>
    </td>
  </tr>
  <tr>
    <td>Apr 7 (Mon)</td>
    <td><strong>Building Reasoning Models & Systems I</strong><br>
    <ul>
      <li><a href="https://arxiv.org/abs/2201.11903">Chain of Thought Prompting Elicits Reasoning in Large Language Models</a></li>
      <li><a href="https://arxiv.org/abs/2203.11171">Self-Consistency Improves Chain of Thought Reasoning in Language Models</a></li>
      <li><a href="https://arxiv.org/abs/2203.14465">STAR: Bootstrapping Reasoning With Reasoning</a></li>
    </ul>
    </td>
  </tr>
  <tr>
    <td>Apr 9 (Wed)</td>
    <td><strong>Building Reasoning Models & Systems II</strong><br>
    <ul>
      <li><a href="https://openai.com/index/o3-mini-system-card/">OpenAI o3-mini System Card</a></li>
      <li><a href="https://arxiv.org/pdf/2501.12948">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via
        Reinforcement Learning</a></li>
      <li><a href="https://arxiv.org/pdf/2411.15124">TÃ¼lu 3: Pushing Frontiers in
        Open Language Model Post-Training</a></li>
    </ul>
    </td>
  </tr>
  <tr>
    <td>Apr 14 (Mon)</td>
    <td><strong>AI Agents</strong><br>
    <ul>
      <li><a href="https://arxiv.org/abs/2405.15793">SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering</a></li>
      <li><a href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting in Language Models</a></li>
      <li><a href="https://arxiv.org/abs/2503.04625">START: Self-taught Reasoner with Tools</a></li>
    </ul>
    </td>
  </tr>
  <tr>
    <td>Apr 16 (Wed)</td>
    <td><strong>Deep Research</strong><br>
    <ul>
      <li><a href="https://cdn.openai.com/deep-research-system-card.pdf">Deep Research System Card</a></li>
      <li><a href="https://arxiv.org/abs/2401.12475">Search-R1: Training LLMs to Reason and Leverage Search Engines</a></li>
      <li><a href="https://arxiv.org/abs/2402.04867">OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs</a></li>
    </ul>
    </td>
  </tr>
  <tr>
    <td>Apr 21 (Mon)</td>
    <td><strong>Evaluation and Benchmarks I</strong><br>
    <ul>
      <li><a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/5d413e48f84dc61244b6be550f1cd8f5-Abstract-Datasets_and_Benchmarks_Track.html">OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments</a></li>
      <li><a href="https://arxiv.org/abs/2307.13854">WebArena: A Realistic Web Environment for Building Autonomous Agents</a></li>
      <li><a href="https://arxiv.org/abs/2403.04132">Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference</a></li>
    </ul>
    </td>
  </tr>
  <tr>
    <td>Apr 23 (Wed)</td>
    <td><strong>Evaluation and Benchmarks II</strong><br>
    <ul>
      <li><a href="https://arxiv.org/abs/2311.12022">GPQA: A Graduate-Level Google-Proof Q&A Benchmark</a></li>
      <li><a href="https://arxiv.org/abs/2103.03874">Measuring Mathematical Problem Solving With the MATH Dataset</a></li>
      <li><a href="https://arxiv.org/abs/2403.07974v2">LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code</a></li>
    </ul>
    </td>
  </tr>
  <tr>
    <td>Apr 28 (Mon)</td>
    <td><strong>Features and Limitations I</strong><br>
    <ul>
      <li><a href="https://www.nature.com/articles/s41586-024-07566-y">AI models collapse when trained on recursively generated data</a></li>
      <li><a href="https://arxiv.org/abs/2309.12288">The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"</a></li>
      <li><a href="https://arxiv.org/abs/2311.00059">The Generative AI Paradox: "What It Can Create, It May Not Understand"</a></li>
    </ul>
    </td>
  </tr>
  <tr>
    <td>Apr 30 (Wed)</td>
    <td><strong>Features and Limitations II</strong><br>
    <ul>
      <li><a href="https://arxiv.org/abs/2412.14093">Alignment faking in large language models</a></li>
      <li><a href="https://arxiv.org/abs/2201.02177">Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets</a></li>
      <li><a href="https://arxiv.org/abs/2412.04318">The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation</a></li>
    </ul>
    </td>
  </tr>
  <tr>
    <td>May 5 (Mon)</td>
    <td><strong>Alternative Architectures</strong><br>
    <ul>
      <li><a href="https://arxiv.org/abs/2312.00752">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a></li>
      <li><a href="https://arxiv.org/abs/2407.04620">Learning to (Learn at Test Time): RNNs with Expressive Hidden States</a></li>
      <li><a href="https://arxiv.org/abs/2412.08821">Large Concept Models: Language Modeling in a Sentence Representation Space</a></li>
    </ul>
    </td>
  </tr>
  <tr>
    <td>May 7 (Wed)</td>
    <td><strong>Efficiency and Scaling</strong><br>
    <ul>
      <li><a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention</a></li>
      <li><a href="https://arxiv.org/pdf/2309.06180">vLLM: Easy, Fast, and Cheap LLM Serving</a></li>
      <li><a href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of Quantized LLMs</a></li>
    </ul>
    </td>
  </tr>
  <tr>
    <td>May 12 (Mon)</td>
    <td><strong>Pretraining Data I</strong><br>
    <ul>
      <li><a href="https://arxiv.org/abs/2406.11794">DataComp-LM: Next Generation Training Sets</a></li>
      <li><a href="https://arxiv.org/abs/2402.00159">Dolma: Open Corpus of Three Trillion Tokens</a></li>
      <li><a href="https://arxiv.org/abs/2411.12372">RedPajama: Reproducing LLaMA Training Dataset</a></li>
    </ul>
    </td>
  </tr>
  <tr>
    <td>May 14 (Wed)</td>
    <td><strong>Pretraining Data II</strong><br>
    <ul>
      <li><a href="https://arxiv.org/abs/2310.20707">What's In My Big Data?</a></li>
      <li><a href="https://arxiv.org/abs/2401.17377">Infini-gram: Scaling Unbounded n-gram Language Models</a></li>
      <li><a href="https://arxiv.org/abs/2407.14933">Consent in Crisis: The Rapid Decline of the AI Data Commons</a></li>
    </ul>
    </td>
  </tr>
  <tr>
    <td>May 19 (Mon)</td>
    <td><strong>TBD</strong></td>
  </tr>
  <tr>
    <td>May 21 (Wed)</td>
    <td><strong>TBD</strong></td>
  </tr>
  <tr>
    <td>May 26 (Mon)</td>
    <td><strong>TBD</strong></td>
  </tr>
  <tr>
    <td>May 28 (Wed)</td>
    <td><strong>TBD</strong></td>
  </tr>
  <tr>
    <td>Jun 2 (Mon)</td>
    <td><strong>TBD</strong></td>
  </tr>
  <tr>
    <td>Jun 4 (Wed)</td>
    <td><strong>Final Project Presentations</strong></td>
  </tr>
</table>

<hr/>
<h3>Acknowledgements</h3>
<!-- We are grateful to Sewon Min, Rulin Shao, Ludwig Schmidt, and Tatsunori Hashimoto for their feedback and assistance with this course. -->
